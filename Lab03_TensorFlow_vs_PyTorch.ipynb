{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e9adb2-38e0-4657-ab24-2a585256d9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8593 - loss: 0.5044\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9530 - loss: 0.1613\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1083\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0808\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.0654\n",
            "TF Training time: 29.23 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.1041\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08797445148229599, 0.9739000201225281]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build a simple feedforward neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),             # Input shape matching MNIST image size\n",
        "    tf.keras.layers.Flatten(),                         # Flatten 28x28 images to 1D vectors\n",
        "    tf.keras.layers.Dense(64, activation='relu'),      # Hidden layer with 64 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')    # Output layer with 10 neurons (one per digit class)\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model and measure training time\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")     # Print the training duration\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "outputId": "b2ebea46-33dc-43a3-e6b5-48ae8a1aee6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpmyj6dfr7'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132709103118160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132709100014672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132709103122000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132709100015056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "outputId": "014d65c2-bba2-4a62-b547-1d8bf78f6b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.10MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 246kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.41MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 53.00 seconds\n",
            "Test accuracy: 0.9724\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Transform: convert to tensor and flatten 28x28 image to a 784-dimensional vector\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
        "])\n",
        "\n",
        "# Data loaders with batch sizes\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
        "    batch_size=32\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, transform=transform, download=True),\n",
        "    batch_size=1000\n",
        ")\n",
        "\n",
        "# Define a simple fully connected neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)   # Input layer: 784 input features (28x28), 128 hidden units\n",
        "        self.fc2 = nn.Linear(128, 10)    # Output layer: 128 hidden units -> 10 output classes (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))          # Apply ReLU activation after first linear layer\n",
        "        return self.fc2(x)               # Output layer without activation (CrossEntropyLoss handles softmax)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())      # Adam optimizer for parameter updates\n",
        "loss_fn = nn.CrossEntropyLoss()                 # Cross-entropy loss for classification\n",
        "\n",
        "# Training loop with time measurement\n",
        "start = time.time()\n",
        "for epoch in range(5):                          # Train for 5 epochs\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()                   # Reset gradients\n",
        "        pred = model(x)                         # Forward pass\n",
        "        loss = loss_fn(pred, y)                 # Compute loss\n",
        "        loss.backward()                         # Backpropagation\n",
        "        optimizer.step()                        # Update weights\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")  # Output training duration\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():                           # Disable gradient calculation during inference\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)                 # Get predicted class\n",
        "        correct += (pred == y).sum().item()     # Count correct predictions\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")  # Output test accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "outputId": "582b4d15-cd61-4dab-a177-581c96fa3b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/17.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/17.6 MB\u001b[0m \u001b[31m189.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14.4/17.6 MB\u001b[0m \u001b[31m231.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m243.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255.0    # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0      # Normalize pixel values to [0, 1]\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets using tf.data API\n",
        "batch_size = 32    # Use same batch size as in first TensorFlow example\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "    .shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define a simple Sequential model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),           # Input shape matches MNIST image dimensions\n",
        "    tf.keras.layers.Flatten(),                       # Flatten 28x28 into a 784-dim vector\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Hidden layer with 64 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer with 10 neurons (digit classes) and softmax\n",
        "])\n",
        "\n",
        "# Set up training components\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()      # Categorical crossentropy for multi-class classification\n",
        "optimizer = tf.keras.optimizers.Adam()                   # Adam optimizer\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()  # Training accuracy metric\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()   # Testing accuracy metric\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)       # Forward pass\n",
        "            loss = loss_fn(y_batch, logits)              # Compute loss\n",
        "        grads = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update weights\n",
        "        train_acc_metric.update_state(y_batch, logits)   # Update training accuracy\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch + 1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Custom evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)     # Forward pass in inference mode\n",
        "    test_acc_metric.update_state(y_batch, test_logits)  # Update test accuracy metric\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "outputId": "c709e9bd-b85f-4f06-813e-db5bb6e69696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3851, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.4746, Accuracy: 0.7262\n",
            "Step 200, Loss: 0.4701, Accuracy: 0.8010\n",
            "Step 300, Loss: 0.2824, Accuracy: 0.8278\n",
            "Step 400, Loss: 0.2232, Accuracy: 0.8472\n",
            "Step 500, Loss: 0.2614, Accuracy: 0.8610\n",
            "Step 600, Loss: 0.3206, Accuracy: 0.8695\n",
            "Step 700, Loss: 0.2948, Accuracy: 0.8763\n",
            "Step 800, Loss: 0.2484, Accuracy: 0.8824\n",
            "Step 900, Loss: 0.4715, Accuracy: 0.8872\n",
            "Step 1000, Loss: 0.3232, Accuracy: 0.8909\n",
            "Step 1100, Loss: 0.1217, Accuracy: 0.8939\n",
            "Step 1200, Loss: 0.1202, Accuracy: 0.8970\n",
            "Step 1300, Loss: 0.1591, Accuracy: 0.8996\n",
            "Step 1400, Loss: 0.1314, Accuracy: 0.9019\n",
            "Step 1500, Loss: 0.3380, Accuracy: 0.9045\n",
            "Step 1600, Loss: 0.1207, Accuracy: 0.9070\n",
            "Step 1700, Loss: 0.2029, Accuracy: 0.9094\n",
            "Step 1800, Loss: 0.1129, Accuracy: 0.9114\n",
            "Training Accuracy for epoch 1: 0.9128\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.3366, Accuracy: 0.8750\n",
            "Step 100, Loss: 0.1211, Accuracy: 0.9508\n",
            "Step 200, Loss: 0.0795, Accuracy: 0.9515\n",
            "Step 300, Loss: 0.4939, Accuracy: 0.9520\n",
            "Step 400, Loss: 0.1132, Accuracy: 0.9518\n",
            "Step 500, Loss: 0.2840, Accuracy: 0.9519\n",
            "Step 600, Loss: 0.2906, Accuracy: 0.9526\n",
            "Step 700, Loss: 0.0502, Accuracy: 0.9534\n",
            "Step 800, Loss: 0.1114, Accuracy: 0.9530\n",
            "Step 900, Loss: 0.2106, Accuracy: 0.9533\n",
            "Step 1000, Loss: 0.2042, Accuracy: 0.9535\n",
            "Step 1100, Loss: 0.0749, Accuracy: 0.9531\n",
            "Step 1200, Loss: 0.2045, Accuracy: 0.9536\n",
            "Step 1300, Loss: 0.2464, Accuracy: 0.9537\n",
            "Step 1400, Loss: 0.1363, Accuracy: 0.9545\n",
            "Step 1500, Loss: 0.1021, Accuracy: 0.9546\n",
            "Step 1600, Loss: 0.0626, Accuracy: 0.9547\n",
            "Step 1700, Loss: 0.1309, Accuracy: 0.9552\n",
            "Step 1800, Loss: 0.0243, Accuracy: 0.9558\n",
            "Training Accuracy for epoch 2: 0.9563\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1449, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0980, Accuracy: 0.9688\n",
            "Step 200, Loss: 0.0757, Accuracy: 0.9689\n",
            "Step 300, Loss: 0.1016, Accuracy: 0.9683\n",
            "Step 400, Loss: 0.2858, Accuracy: 0.9677\n",
            "Step 500, Loss: 0.2285, Accuracy: 0.9674\n",
            "Step 600, Loss: 0.1362, Accuracy: 0.9665\n",
            "Step 700, Loss: 0.0657, Accuracy: 0.9665\n",
            "Step 800, Loss: 0.2053, Accuracy: 0.9666\n",
            "Step 900, Loss: 0.1601, Accuracy: 0.9670\n",
            "Step 1000, Loss: 0.0967, Accuracy: 0.9671\n",
            "Step 1100, Loss: 0.1234, Accuracy: 0.9665\n",
            "Step 1200, Loss: 0.3275, Accuracy: 0.9667\n",
            "Step 1300, Loss: 0.1064, Accuracy: 0.9663\n",
            "Step 1400, Loss: 0.0401, Accuracy: 0.9667\n",
            "Step 1500, Loss: 0.1977, Accuracy: 0.9665\n",
            "Step 1600, Loss: 0.4622, Accuracy: 0.9666\n",
            "Step 1700, Loss: 0.1621, Accuracy: 0.9669\n",
            "Step 1800, Loss: 0.0285, Accuracy: 0.9675\n",
            "Training Accuracy for epoch 3: 0.9677\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0482, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.2924, Accuracy: 0.9715\n",
            "Step 200, Loss: 0.1445, Accuracy: 0.9726\n",
            "Step 300, Loss: 0.0193, Accuracy: 0.9747\n",
            "Step 400, Loss: 0.1049, Accuracy: 0.9737\n",
            "Step 500, Loss: 0.0107, Accuracy: 0.9737\n",
            "Step 600, Loss: 0.0234, Accuracy: 0.9730\n",
            "Step 700, Loss: 0.1533, Accuracy: 0.9725\n",
            "Step 800, Loss: 0.0123, Accuracy: 0.9728\n",
            "Step 900, Loss: 0.2062, Accuracy: 0.9732\n",
            "Step 1000, Loss: 0.1523, Accuracy: 0.9730\n",
            "Step 1100, Loss: 0.0546, Accuracy: 0.9735\n",
            "Step 1200, Loss: 0.0396, Accuracy: 0.9732\n",
            "Step 1300, Loss: 0.1088, Accuracy: 0.9732\n",
            "Step 1400, Loss: 0.1145, Accuracy: 0.9733\n",
            "Step 1500, Loss: 0.0562, Accuracy: 0.9734\n",
            "Step 1600, Loss: 0.0353, Accuracy: 0.9736\n",
            "Step 1700, Loss: 0.0204, Accuracy: 0.9736\n",
            "Step 1800, Loss: 0.0800, Accuracy: 0.9736\n",
            "Training Accuracy for epoch 4: 0.9737\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.3118, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0348, Accuracy: 0.9811\n",
            "Step 200, Loss: 0.2411, Accuracy: 0.9792\n",
            "Step 300, Loss: 0.1385, Accuracy: 0.9781\n",
            "Step 400, Loss: 0.0677, Accuracy: 0.9784\n",
            "Step 500, Loss: 0.0382, Accuracy: 0.9788\n",
            "Step 600, Loss: 0.0421, Accuracy: 0.9784\n",
            "Step 700, Loss: 0.0604, Accuracy: 0.9783\n",
            "Step 800, Loss: 0.0192, Accuracy: 0.9785\n",
            "Step 900, Loss: 0.1625, Accuracy: 0.9789\n",
            "Step 1000, Loss: 0.0272, Accuracy: 0.9791\n",
            "Step 1100, Loss: 0.0765, Accuracy: 0.9789\n",
            "Step 1200, Loss: 0.0409, Accuracy: 0.9787\n",
            "Step 1300, Loss: 0.2131, Accuracy: 0.9784\n",
            "Step 1400, Loss: 0.0309, Accuracy: 0.9781\n",
            "Step 1500, Loss: 0.0166, Accuracy: 0.9778\n",
            "Step 1600, Loss: 0.0233, Accuracy: 0.9780\n",
            "Step 1700, Loss: 0.0191, Accuracy: 0.9782\n",
            "Step 1800, Loss: 0.0277, Accuracy: 0.9783\n",
            "Training Accuracy for epoch 5: 0.9784\n",
            "\n",
            "TF Training time: 354.42 seconds\n",
            "Test Accuracy: 0.9715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0     # Normalize pixel values to [0, 1]\n",
        "y_train = to_categorical(y_train, num_classes=10)  # One-hot encode labels\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),          # MNIST image shape\n",
        "    tf.keras.layers.Flatten(),                      # Flatten to 784-dim vector\n",
        "    tf.keras.layers.Dense(64, activation='relu'),   # Hidden layer: 64 neurons, ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # Output layer: 10 classes, softmax activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch + 1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "outputId": "88c09f00-960a-48ed-8820-e8a3ac014b75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jmu_hciK_qle",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3430, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.3737, Accuracy: 0.7169\n",
            "Step 200, Loss: 0.3423, Accuracy: 0.7956\n",
            "Step 300, Loss: 0.4230, Accuracy: 0.8300\n",
            "Step 400, Loss: 1.0134, Accuracy: 0.8490\n",
            "Step 500, Loss: 0.4091, Accuracy: 0.8620\n",
            "Step 600, Loss: 0.3783, Accuracy: 0.8706\n",
            "Step 700, Loss: 0.0889, Accuracy: 0.8768\n",
            "Step 800, Loss: 0.2109, Accuracy: 0.8821\n",
            "Step 900, Loss: 0.5630, Accuracy: 0.8871\n",
            "Step 1000, Loss: 0.5430, Accuracy: 0.8916\n",
            "Step 1100, Loss: 0.3470, Accuracy: 0.8948\n",
            "Step 1200, Loss: 0.3125, Accuracy: 0.8979\n",
            "Step 1300, Loss: 0.3782, Accuracy: 0.9012\n",
            "Step 1400, Loss: 0.3207, Accuracy: 0.9044\n",
            "Step 1500, Loss: 0.1462, Accuracy: 0.9065\n",
            "Step 1600, Loss: 0.2320, Accuracy: 0.9095\n",
            "Step 1700, Loss: 0.0461, Accuracy: 0.9116\n",
            "Step 1800, Loss: 0.2461, Accuracy: 0.9139\n",
            "Training Accuracy for epoch 1: 0.9154\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1203, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.3289, Accuracy: 0.9554\n",
            "Step 200, Loss: 0.1423, Accuracy: 0.9524\n",
            "Step 300, Loss: 0.4255, Accuracy: 0.9518\n",
            "Step 400, Loss: 0.1027, Accuracy: 0.9522\n",
            "Step 500, Loss: 0.0640, Accuracy: 0.9536\n",
            "Step 600, Loss: 0.0954, Accuracy: 0.9536\n",
            "Step 700, Loss: 0.0292, Accuracy: 0.9544\n",
            "Step 800, Loss: 0.0517, Accuracy: 0.9555\n",
            "Step 900, Loss: 0.0806, Accuracy: 0.9558\n",
            "Step 1000, Loss: 0.2616, Accuracy: 0.9551\n",
            "Step 1100, Loss: 0.0536, Accuracy: 0.9559\n",
            "Step 1200, Loss: 0.1970, Accuracy: 0.9560\n",
            "Step 1300, Loss: 0.0980, Accuracy: 0.9559\n",
            "Step 1400, Loss: 0.0368, Accuracy: 0.9555\n",
            "Step 1500, Loss: 0.2079, Accuracy: 0.9558\n",
            "Step 1600, Loss: 0.1071, Accuracy: 0.9564\n",
            "Step 1700, Loss: 0.1115, Accuracy: 0.9568\n",
            "Step 1800, Loss: 0.2042, Accuracy: 0.9572\n",
            "Training Accuracy for epoch 2: 0.9576\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0828, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1122, Accuracy: 0.9663\n",
            "Step 200, Loss: 0.0446, Accuracy: 0.9669\n",
            "Step 300, Loss: 0.1230, Accuracy: 0.9673\n",
            "Step 400, Loss: 0.2474, Accuracy: 0.9681\n",
            "Step 500, Loss: 0.1203, Accuracy: 0.9679\n",
            "Step 600, Loss: 0.0695, Accuracy: 0.9692\n",
            "Step 700, Loss: 0.0676, Accuracy: 0.9696\n",
            "Step 800, Loss: 0.0869, Accuracy: 0.9693\n",
            "Step 900, Loss: 0.0347, Accuracy: 0.9691\n",
            "Step 1000, Loss: 0.0918, Accuracy: 0.9691\n",
            "Step 1100, Loss: 0.0248, Accuracy: 0.9691\n",
            "Step 1200, Loss: 0.0182, Accuracy: 0.9691\n",
            "Step 1300, Loss: 0.0431, Accuracy: 0.9690\n",
            "Step 1400, Loss: 0.2779, Accuracy: 0.9691\n",
            "Step 1500, Loss: 0.1728, Accuracy: 0.9692\n",
            "Step 1600, Loss: 0.1553, Accuracy: 0.9688\n",
            "Step 1700, Loss: 0.0235, Accuracy: 0.9690\n",
            "Step 1800, Loss: 0.1792, Accuracy: 0.9694\n",
            "Training Accuracy for epoch 3: 0.9694\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0844, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0300, Accuracy: 0.9762\n",
            "Step 200, Loss: 0.0856, Accuracy: 0.9739\n",
            "Step 300, Loss: 0.1205, Accuracy: 0.9743\n",
            "Step 400, Loss: 0.1868, Accuracy: 0.9750\n",
            "Step 500, Loss: 0.0091, Accuracy: 0.9757\n",
            "Step 600, Loss: 0.1114, Accuracy: 0.9755\n",
            "Step 700, Loss: 0.0442, Accuracy: 0.9750\n",
            "Step 800, Loss: 0.0519, Accuracy: 0.9750\n",
            "Step 900, Loss: 0.1417, Accuracy: 0.9747\n",
            "Step 1000, Loss: 0.3593, Accuracy: 0.9750\n",
            "Step 1100, Loss: 0.0331, Accuracy: 0.9749\n",
            "Step 1200, Loss: 0.0278, Accuracy: 0.9750\n",
            "Step 1300, Loss: 0.1004, Accuracy: 0.9751\n",
            "Step 1400, Loss: 0.1705, Accuracy: 0.9751\n",
            "Step 1500, Loss: 0.0450, Accuracy: 0.9748\n",
            "Step 1600, Loss: 0.0078, Accuracy: 0.9749\n",
            "Step 1700, Loss: 0.0057, Accuracy: 0.9750\n",
            "Step 1800, Loss: 0.1181, Accuracy: 0.9752\n",
            "Training Accuracy for epoch 4: 0.9752\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.1916, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0260, Accuracy: 0.9821\n",
            "Step 200, Loss: 0.0969, Accuracy: 0.9824\n",
            "Step 300, Loss: 0.0165, Accuracy: 0.9814\n",
            "Step 400, Loss: 0.0182, Accuracy: 0.9813\n",
            "Step 500, Loss: 0.0815, Accuracy: 0.9809\n",
            "Step 600, Loss: 0.0270, Accuracy: 0.9811\n",
            "Step 700, Loss: 0.0548, Accuracy: 0.9813\n",
            "Step 800, Loss: 0.1436, Accuracy: 0.9807\n",
            "Step 900, Loss: 0.1602, Accuracy: 0.9805\n",
            "Step 1000, Loss: 0.2021, Accuracy: 0.9804\n",
            "Step 1100, Loss: 0.0931, Accuracy: 0.9804\n",
            "Step 1200, Loss: 0.0684, Accuracy: 0.9803\n",
            "Step 1300, Loss: 0.0787, Accuracy: 0.9798\n",
            "Step 1400, Loss: 0.2741, Accuracy: 0.9794\n",
            "Step 1500, Loss: 0.0455, Accuracy: 0.9798\n",
            "Step 1600, Loss: 0.0174, Accuracy: 0.9799\n",
            "Step 1700, Loss: 0.0624, Accuracy: 0.9800\n",
            "Step 1800, Loss: 0.0037, Accuracy: 0.9800\n",
            "Training Accuracy for epoch 5: 0.9799\n",
            "\n",
            "TF Training time: 31.37 seconds\n",
            "Test Accuracy: 0.9728\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}